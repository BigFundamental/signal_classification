{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "from script.feature_extractor import FeatureExtractor\n",
    "from script.classifier import Classifier\n",
    "from script.signal_manager import SignalMgr\n",
    "from script.filter import Filter\n",
    "from script.data_reader import DataReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INVALID_SLIGHT_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_轻微.20190515/'\n",
    "INVALID_BAD_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "FULL_DATA_FAPTH='/Volumes/workspace/projects/signal_classification/data/1005_0830重新标注文件_Data._20180609.0830'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = DataReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180325_090536</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180325_090637</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180325_091016</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180325_091047</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180325_091103</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         case_name  channel_id  \\\n",
       "0  20180325_090536           1   \n",
       "1  20180325_090637           1   \n",
       "2  20180325_091016           1   \n",
       "3  20180325_091047           1   \n",
       "4  20180325_091103           1   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              0      -1  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1       9  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1       5  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1       5  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1       5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df = data_reader.create_single_index(FULL_DATA_FAPTH+'/'+'result.csv').drop(labels='sys_result', axis=1)\n",
    "norm_df[norm_df.reason==4].describe()\n",
    "norm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190515_204352497           1   \n",
       "1  20190515_204352497           2   \n",
       "2  20190515_204352497           3   \n",
       "3  20190515_204352497           4   \n",
       "4  20190515_204352497           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      61  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_slight_df = data_reader.get_signal_list(INVALID_SLIGHT_DATA_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "invalid_slight_df['expect_result'] = 1\n",
    "invalid_slight_df['reason'] = 61\n",
    "invalid_slight_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190515_203431979           1   \n",
       "1  20190515_203431979           2   \n",
       "2  20190515_203431979           3   \n",
       "3  20190515_203431979           4   \n",
       "4  20190515_203431979           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      62  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_bad_df = data_reader.get_signal_list(INVALID_BAD_DATA_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "invalid_bad_df['expect_result'] = 1\n",
    "invalid_bad_df['reason'] = 62\n",
    "invalid_bad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigMgr = SignalMgr()\n",
    "# feature = sigMgr.get_features(path1, request_param={'skip_row':[1], 'model_path':['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始进行模型迭代和训练, 整合训练数据和测试数据\n",
    "\n",
    "def data_prepare(train_path, test_path, eval_path):\n",
    "    # 预留长短波形的数据用于数据的测试\n",
    "    msk = np.random.rand(len(invalid_bad_df)) < 0.8\n",
    "    invalid_bad_train_df = invalid_bad_df[msk]         #用于训练\n",
    "    invalid_bad_eval_df = invalid_bad_df[~msk]         #用于最后验证\n",
    "    \n",
    "    msk = np.random.rand(len(invalid_slight_df)) < 0.8\n",
    "    invalid_slight_train_df = invalid_slight_df[msk]\n",
    "    invalid_slight_eval_df = invalid_slight_df[~msk]\n",
    "    \n",
    "    eval_mix_df = invalid_bad_eval_df.append(invalid_slight_eval_df).reset_index(drop=True)\n",
    "    # 获取整体的训练数据\n",
    "    train_mix_df = invalid_slight_train_df.append(invalid_bad_train_df).reset_index(drop=True)\n",
    "    # 再次划分为测试集合与训练集合\n",
    "    msk = np.random.rand(len(train_mix_df)) < 0.8\n",
    "    train_df = train_mix_df[msk]\n",
    "    test_df = train_mix_df[~msk]\n",
    "\n",
    "    # pandas 写入到文件中进行缓存，用于迭代测试，避免出现每次划分数据集合auc发生变化\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    eval_mix_df.to_csv(eval_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_prepare(\"../data/train_skew.csv\", \"../data/test_skew.csv\", \"../data/eval_skew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = '../data'\n",
    "# 开始进行特征的获取\n",
    "train_tmp_df = pd.read_csv(data_root + '/' + 'train.csv')\n",
    "train_skew_df = pd.read_csv(data_root + '/' + 'train_skew.csv')\n",
    "train_df = train_tmp_df.append(train_skew_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tmp_df = pd.read_csv(data_root + '/' + 'test.csv')\n",
    "test_skew_df = pd.read_csv(data_root + '/' + 'test_skew.csv')\n",
    "test_df = test_tmp_df.append(test_skew_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_cases = train_df[train_df.expect_result == 0]\n",
    "other_defect_cases = train_df[(train_df.expect_result == 1) & (train_df.reason != 6) & (train_df.reason != 61) & (train_df.reason != 62)]\n",
    "defet_cases = train_df[(train_df.reason == 6) | (train_df.reason == 61) | (train_df.reason == 62)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    mu_list = []\n",
    "    delta_list = []\n",
    "    sigMgr = SignalMgr()\n",
    "    for path in df['case_path']:\n",
    "        feature = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "        mu_list.append(np.mean(feature['unit_interviene_length_diff']))\n",
    "        delta_list.append(np.std(feature['unit_interviene_length_diff']))\n",
    "    return np.min(mu_list), np.max(mu_list), np.min(delta_list), np.max(delta_list), np.mean(mu_list), np.mean(delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 整体数据的区分很大，因此介入进行数据分析和训练\n",
    "feature_names = ['peaks_num', 'down_peaks_num', 'up_edges_num', 'down_edges_num', 'peak_edge_ratio', 'down_peak_edge_ratio',\n",
    "                 'edge_diff_10', 'edge_diff_20', 'width_diff_10', 'negative_peak_num', 'max_down_peak_point', 'inter_diff_mean', 'inter_diff_delta',\n",
    "                'skewness_mean', 'skewness_delta']\n",
    "                \n",
    "def features(df_full, feature_names):\n",
    "    pathes = df_full['case_path']\n",
    "    # print pathes\n",
    "    feature_set = dict()\n",
    "    for name in feature_names:\n",
    "        feature_set[name] = list()\n",
    "#     feature_set['inter_diff_mean'] = list()\n",
    "#     feature_set['inter_diff_delta'] = list()\n",
    "#     feature_set['skewness_mean'] = list()\n",
    "#     feature_set['skewness_delta'] = list()\n",
    "#     feature_set['skewness_median'] = list()\n",
    "#     feature_set['skewness_10'] = list()\n",
    "#     feature_set['skewness_20'] = list()\n",
    "#     feature_set['skewness_30'] = list()\n",
    "#     feature_set['skewness_']\n",
    "    \n",
    "    for test_case in pathes:\n",
    "        features = sigMgr.get_features(test_case, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "        for name in feature_names:\n",
    "            feature_set[name].append(features[name])\n",
    "#         feature_set['inter_diff_mean'].append(np.mean(features['unit_interviene_length_diff']))\n",
    "#         feature_set['inter_diff_delta'].append(np.std(features['unit_interviene_length_diff']))\n",
    "#         skewness_list = sorted(features['unit_interviene_skewness'], reverse=True)\n",
    "#         feature_set['skewness_median'] = np.percentile(skewness_list, 50)\n",
    "#         feature_set['skewness_10'] = np.percentile(skewness_list, 90)\n",
    "#         feature_set['skewness_20'] = np.percentile(skewness_list, 80)\n",
    "#         feature_set['skewness_30'] = np.percentile(skewness_list, 70)\n",
    "#         feature_set['skewness_mean'].append(np.mean(features['unit_interviene_skewness']))\n",
    "#         feature_set['skewness_delta'].append(np.std(features['unit_interviene_skewness']))\n",
    "    \n",
    "    return pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train_x = features(train_df, feature_names)\n",
    "train_y = train_df['expect_result']\n",
    "test_x = features(test_df, feature_names)\n",
    "test_y = test_df['expect_result']\n",
    "\n",
    "train_y[train_y == -1] = 0\n",
    "test_y[test_y == -1] = 0\n",
    "test_x = test_x.fillna(0)\n",
    "train_x = train_x.fillna(0)\n",
    "# test_df = pd.read_csv(data_root + '/' + 'test.csv')\n",
    "# test_x = features(test_df, feature_names)\n",
    "# test_y = test_df['expect_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90       632\n",
      "           1       0.94      0.95      0.94      1093\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1725\n",
      "   macro avg       0.92      0.92      0.92      1725\n",
      "weighted avg       0.93      0.93      0.93      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier as ada\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gdbtModel = GradientBoostingClassifier()\n",
    "gdbtModel.fit(train_x, train_y)\n",
    "pResult = gdbtModel.predict(test_x)\n",
    "print(classification_report(test_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.79      0.88        76\n",
      "\n",
      "   micro avg       0.79      0.79      0.79        76\n",
      "   macro avg       0.50      0.39      0.44        76\n",
      "weighted avg       1.00      0.79      0.88        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.read_csv(data_root + '/' + 'eval_skew.csv')\n",
    "eval_df_x = features(eval_df, feature_names)\n",
    "eval_df_y = eval_df['expect_result']\n",
    "\n",
    "pResult = gdbtModel.predict(eval_df_x)\n",
    "print(classification_report(eval_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.80      0.89        50\n",
      "\n",
      "   micro avg       0.80      0.80      0.80        50\n",
      "   macro avg       0.50      0.40      0.44        50\n",
      "weighted avg       1.00      0.80      0.89        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hard_eval_df = eval_df[eval_df.reason == 61]\n",
    "eval_df_x = features(hard_eval_df, feature_names)\n",
    "eval_df_y = hard_eval_df['expect_result']\n",
    "pResult = gdbtModel.predict(eval_df_x)\n",
    "print(classification_report(eval_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.77      0.87        26\n",
      "\n",
      "   micro avg       0.77      0.77      0.77        26\n",
      "   macro avg       0.50      0.38      0.43        26\n",
      "weighted avg       1.00      0.77      0.87        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hard_eval_df = eval_df[eval_df.reason == 62]\n",
    "eval_df_x = features(hard_eval_df, feature_names)\n",
    "eval_df_y = hard_eval_df['expect_result']\n",
    "pResult = gdbtModel.predict(eval_df_x)\n",
    "print(classification_report(eval_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始斜边的波形调研，先看用现有的基线能得到多好的测试结果\n",
    "SKEW_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "data_reader = DataReader()\n",
    "skew_angel_df = data_reader.get_signal_list(SKEW_DATA_FPATH)\n",
    "skew_angel_df['expect_result'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = skew_angel_df.head()['case_path'][4]\n",
    "signals = pd.read_csv(path, skiprows=1)\n",
    "signals[200:220].plot()\n",
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "# medfiltered_signals = Filter.medfilter(normalized_signals, 9)\n",
    "print(np.mean(feas['unit_interviene_skewness']))\n",
    "print(np.std(feas['unit_interviene_skewness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "# medfiltered_signals = Filter.medfilter(normalized_signals, 9)\n",
    "print(np.mean(feas['unit_interviene_skewness']))\n",
    "print(np.std(feas['unit_interviene_skewness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = normal_cases['case_path'].reset_index(drop=True)[6]\n",
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "normalized_signals = feas['normalized_signals'] \n",
    "medfiltered_signals = Filter.medfilter(normalized_signals, 5)\n",
    "pd.DataFrame(medfiltered_signals)[0:500].plot()\n",
    "print(np.mean(feas['unit_interviene_skewness']))\n",
    "print(np.std(feas['unit_interviene_skewness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始斜边的波形调研，先看用现有的基线能得到多好的测试结果\n",
    "SKEW_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "data_reader = DataReader()\n",
    "skew_angel_df = data_reader.get_signal_list(SKEW_DATA_FPATH)\n",
    "skew_angel_df['expect_result'] = 1\n",
    "skew_unit_skewness_list = []\n",
    "for path in skew_angel_df['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    skew_unit_skewness_list.extend(feas['unit_interviene_skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (np.mean(skew_unit_skewness_list), np.std(skew_unit_skewness_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SKEW_DATA_FPATH_LIGHT='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_轻微.20190515/'\n",
    "skew_light_angel_df = data_reader.get_signal_list(SKEW_DATA_FPATH_LIGHT)\n",
    "skew_light_angel_df['expect_result'] = 1\n",
    "skew_unit_skewness_list = []\n",
    "for path in skew_light_angel_df['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    count = 0\n",
    "    for angel in feas['unit_interviene_skewness']:\n",
    "        if angel > (0.0124 + 1 * 0.035):\n",
    "            count+=1\n",
    "    skew_unit_skewness_list.append(count)\n",
    "#     skew_unit_skewness_list.extend(feas['unit_interviene_skewness'])\n",
    "print (np.mean(skew_unit_skewness_list), np.std(skew_unit_skewness_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "new_List = []\n",
    "for val in skew_unit_skewness_list:\n",
    "    if math.isnan(val) or math.isinf(val):\n",
    "        continue\n",
    "    new_List.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (np.mean(new_List), np.std(new_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(new_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goods = []\n",
    "for path in norm_df[norm_df.expect_result == 0]['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    count = 0\n",
    "    for angel in feas['unit_interviene_skewness']:\n",
    "        if angel > (0.0124 + 1 * 0.035):\n",
    "            count+=1\n",
    "    goods.append(count)\n",
    "print (np.mean(goods), np.std(goods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(new_List)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tpy2]",
   "language": "python",
   "name": "conda-env-tpy2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
