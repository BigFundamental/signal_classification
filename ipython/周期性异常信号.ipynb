{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "from script.feature_extractor import FeatureExtractor\n",
    "from script.classifier import Classifier\n",
    "from script.signal_manager import SignalMgr\n",
    "from script.filter import Filter\n",
    "from script.data_reader import DataReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INVALID_SLIGHT_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_轻微.20190515/'\n",
    "INVALID_BAD_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "FULL_DATA_FAPTH='/Volumes/workspace/projects/signal_classification/data/1005_0830重新标注文件_Data._20180609.0830'\n",
    "MISS_LABEL_NORMAL_FPATH='/Users/changkong/project/signal_classification/data/20190623标记/20190623NEW'  # 误分的正样本\n",
    "GLUE_ABNORMAL_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/DATA-胶水/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = DataReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180325_090536</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180325_090637</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180325_091016</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180325_091047</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180325_091103</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         case_name  channel_id  \\\n",
       "0  20180325_090536           1   \n",
       "1  20180325_090637           1   \n",
       "2  20180325_091016           1   \n",
       "3  20180325_091047           1   \n",
       "4  20180325_091103           1   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              0      -1  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1       9  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1       5  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1       5  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1       5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df = data_reader.create_single_index(FULL_DATA_FAPTH+'/'+'result.csv').drop(labels='sys_result', axis=1)\n",
    "norm_df[norm_df.reason==4].describe()\n",
    "norm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190515_204352497           1   \n",
       "1  20190515_204352497           2   \n",
       "2  20190515_204352497           3   \n",
       "3  20190515_204352497           4   \n",
       "4  20190515_204352497           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      61  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_slight_df = data_reader.get_signal_list(INVALID_SLIGHT_DATA_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "invalid_slight_df['expect_result'] = 1\n",
    "invalid_slight_df['reason'] = 61\n",
    "invalid_slight_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190711_144110136</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190711_144110136</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190711_144110136</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190711_144110136</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190711_144110136</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190711_144110136           1   \n",
       "1  20190711_144110136           2   \n",
       "2  20190711_144110136           3   \n",
       "3  20190711_144110136           4   \n",
       "4  20190711_144110136           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      71  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      71  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      71  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      71  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      71  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_df = data_reader.get_signal_list(GLUE_ABNORMAL_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "glue_df['expect_result'] = 1\n",
    "glue_df['reason'] = 71\n",
    "glue_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190515_203431979           1   \n",
       "1  20190515_203431979           2   \n",
       "2  20190515_203431979           3   \n",
       "3  20190515_203431979           4   \n",
       "4  20190515_203431979           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      62  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_bad_df = data_reader.get_signal_list(INVALID_BAD_DATA_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "invalid_bad_df['expect_result'] = 1\n",
    "invalid_bad_df['reason'] = 62\n",
    "invalid_bad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigMgr = SignalMgr()\n",
    "# feature = sigMgr.get_features('xxx', request_param={'skip_row':[1], 'model_path':['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始进行模型迭代和训练, 整合训练数据和测试数据\n",
    "\n",
    "def data_prepare(train_path, test_path, eval_path):\n",
    "    # 预留长短波形的数据用于数据的测试\n",
    "    msk = np.random.rand(len(invalid_bad_df)) < 0.8\n",
    "    invalid_bad_train_df = invalid_bad_df[msk]         #用于训练\n",
    "    invalid_bad_eval_df = invalid_bad_df[~msk]         #用于最后验证\n",
    "    \n",
    "    msk = np.random.rand(len(invalid_slight_df)) < 0.8\n",
    "    invalid_slight_train_df = invalid_slight_df[msk]\n",
    "    invalid_slight_eval_df = invalid_slight_df[~msk]\n",
    "    \n",
    "    eval_mix_df = invalid_bad_eval_df.append(invalid_slight_eval_df).reset_index(drop=True)\n",
    "    # 获取整体的训练数据\n",
    "    train_mix_df = invalid_slight_train_df.append(invalid_bad_train_df).reset_index(drop=True)\n",
    "    # 再次划分为测试集合与训练集合\n",
    "    msk = np.random.rand(len(train_mix_df)) < 0.8\n",
    "    train_df = train_mix_df[msk]\n",
    "    test_df = train_mix_df[~msk]\n",
    "\n",
    "    # pandas 写入到文件中进行缓存，用于迭代测试，避免出现每次划分数据集合auc发生变化\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    eval_mix_df.to_csv(eval_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_prepare(\"../data/train_skew.csv\", \"../data/test_skew.csv\", \"../data/eval_skew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = '../data'\n",
    "# 开始进行特征的获取\n",
    "train_tmp_df = pd.read_csv(data_root + '/' + 'train.csv')\n",
    "train_skew_df = pd.read_csv(data_root + '/' + 'train_skew.csv')\n",
    "train_df = train_tmp_df.append(train_skew_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/workspace/projects/signal_classification/data/特殊次品样本/长短.20190515/20190515_195250029/Channel_2.csv\n"
     ]
    }
   ],
   "source": [
    "print (train_df['case_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>sys_result</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "      <th>channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190623_000136435</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  sys_result  expect_result  reason  channel_id\n",
       "0  20190623_000136435           1              0      -1           8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MISSING_LABEL_DF_FPATH='/Users/changkong/project/signal_classification/data/20190623标记/result_man.csv'\n",
    "missing_classify_possible_df = data_reader.get_signal_list(MISS_LABEL_NORMAL_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "label_df = pd.read_csv(MISSING_LABEL_DF_FPATH, header=None, skiprows=1, names=['case_name', 'sys_result', 'expect_result', 'reason', 'channel_id'])\n",
    "target_df = label_df[label_df.expect_result == 0].reset_index(drop=True)\n",
    "tmp = target_df[['case_name', 'channel_id']].merge(missing_classify_possible_df, on=['case_name', 'channel_id'])\n",
    "missing_classify_possible_df[missing_classify_possible_df.case_name == '20190623_000136435'].head()\n",
    "target_df[target_df.case_name == '20190623_000136435']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_positive_df = tmp\n",
    "missing_positive_df['expect_result'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(missing_positive_df)) < 0.8\n",
    "missing_positive_train_df = missing_positive_df[msk]\n",
    "missing_positive_eval_df = missing_positive_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changkong/anaconda3/envs/tpy2/lib/python2.7/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# missing_positive_train_df.to_csv(data_root + '/' + 'mp_train.csv', index=False)\n",
    "# missing_positive_eval_df.to_csv(data_root + '/' + 'mp_test.csv', index=False)\n",
    "mp_train_df = pd.read_csv(data_root + '/' + 'mp_train.csv')\n",
    "train_df = train_df.append(mp_train_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tmp_df = pd.read_csv(data_root + '/' + 'test.csv')\n",
    "test_skew_df = pd.read_csv(data_root + '/' + 'test_skew.csv')\n",
    "test_mp_df = pd.read_csv(data_root + '/' + 'mp_test.csv')\n",
    "test_df1 = test_tmp_df.append(test_skew_df).reset_index(drop=True)\n",
    "test_df = test_df1.append(test_mp_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_name        261\n",
       "channel_id       261\n",
       "case_path        261\n",
       "expect_result    261\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n",
    "test_mp_df.head()\n",
    "mp_train_df.head()\n",
    "test_mp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_cases = train_df[train_df.expect_result == 0]\n",
    "other_defect_cases = train_df[(train_df.expect_result == 1) & (train_df.reason != 6) & (train_df.reason != 61) & (train_df.reason != 62)]\n",
    "defet_cases = train_df[(train_df.reason == 6) | (train_df.reason == 61) | (train_df.reason == 62)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    mu_list = []\n",
    "    delta_list = []\n",
    "    sigMgr = SignalMgr()\n",
    "    for path in df['case_path']:\n",
    "        feature = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "        mu_list.append(np.mean(feature['unit_interviene_length_diff']))\n",
    "        delta_list.append(np.std(feature['unit_interviene_length_diff']))\n",
    "    return np.min(mu_list), np.max(mu_list), np.min(delta_list), np.max(delta_list), np.mean(mu_list), np.mean(delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 整体数据的区分很大，因此介入进行数据分析和训练\n",
    "feature_names = ['peaks_num', 'down_peaks_num', 'up_edges_num', 'down_edges_num', 'peak_edge_ratio', 'down_peak_edge_ratio',\n",
    "                 'edge_diff_10', 'edge_diff_20', 'width_diff_10', 'negative_peak_num', 'max_down_peak_point', 'inter_diff_mean', 'inter_diff_delta',\n",
    "                'skewness_mean', 'skewness_delta', 'cyclic_intense_nopeak', 'cyclic_intense_downpeak']\n",
    "# feature_names = ['peaks_num', 'down_peaks_num', 'up_edges_num', 'down_edges_num', 'peak_edge_ratio', 'down_peak_edge_ratio',\n",
    "#                  'edge_diff_10', 'edge_diff_20', 'width_diff_10', 'negative_peak_num', 'max_down_peak_point', 'inter_diff_mean', 'inter_diff_delta',\n",
    "#                 'skewness_mean', 'skewness_delta']\n",
    "\n",
    "feature_names = sorted(feature_names, reverse=True)\n",
    "                \n",
    "def features(df_full, feature_names):\n",
    "    pathes = df_full['case_path']\n",
    "    # print pathes\n",
    "    feature_set = dict()\n",
    "    for name in feature_names:\n",
    "        feature_set[name] = list()\n",
    "#     feature_set['cyclic_intense_ratio'] = list()\n",
    "#     feature_set['inter_diff_mean'] = list()\n",
    "#     feature_set['inter_diff_delta'] = list()\n",
    "#     feature_set['skewness_mean'] = list()\n",
    "#     feature_set['skewness_delta'] = list()\n",
    "#     feature_set['skewness_median'] = list()\n",
    "#     feature_set['skewness_10'] = list()\n",
    "#     feature_set['skewness_20'] = list()\n",
    "#     feature_set['skewness_30'] = list()\n",
    "#     feature_set['skewness_']\n",
    "    \n",
    "    for test_case in pathes:\n",
    "        features = sigMgr.get_features(test_case, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "        for name in feature_names:\n",
    "#             if name == 'cyclic_intense_downpeak':\n",
    "#                 if features[name] >= 4:\n",
    "#                     features[name] = 4\n",
    "            feature_set[name].append(features[name])\n",
    "#         feature_set['cyclic_intense_ratio'].append(features['cyclic_intense_downpeak'] / (features['down_peaks_num'] + 0.01))\n",
    "#         feature_set['inter_diff_mean'].append(np.mean(features['unit_interviene_length_diff']))\n",
    "#         feature_set['inter_diff_delta'].append(np.std(features['unit_interviene_length_diff']))\n",
    "#         skewness_list = sorted(features['unit_interviene_skewness'], reverse=True)\n",
    "#         feature_set['skewness_median'] = np.percentile(skewness_list, 50)\n",
    "#         feature_set['skewness_10'] = np.percentile(skewness_list, 90)\n",
    "#         feature_set['skewness_20'] = np.percentile(skewness_list, 80)\n",
    "#         feature_set['skewness_30'] = np.percentile(skewness_list, 70)\n",
    "#         feature_set['skewness_mean'].append(np.mean(features['unit_interviene_skewness']))\n",
    "#         feature_set['skewness_delta'].append(np.std(features['unit_interviene_skewness']))\n",
    "    \n",
    "    return pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = features(train_df, feature_names)\n",
    "train_y = train_df['expect_result']\n",
    "test_x = features(test_df, feature_names)\n",
    "test_y = test_df['expect_result']\n",
    "\n",
    "train_y[train_y == -1] = 0\n",
    "test_y[test_y == -1] = 0\n",
    "test_x = test_x.fillna(0)\n",
    "train_x = train_x.fillna(0)\n",
    "# test_df = pd.read_csv(data_root + '/' + 'test.csv')\n",
    "# test_x = features(test_df, feature_names)\n",
    "# test_y = test_df['expect_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier as ada\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# new_features = ['edge_diff_20', 'inter_diff_mean', 'skewness_mean','down_edges_num', 'cyclic_intense_downpeak', 'peaks_num', 'peak_edge_ratio', 'negative_peak_num', 'down_peaks_num', 'down_peak_edge_ratio']\n",
    "# test_x_tmp = test_x[new_features]\n",
    "# train_x_tmp = train_x[new_features]\n",
    "train_x_tmp = train_x\n",
    "test_x_tmp = test_x\n",
    "gdbtModel = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=3, min_samples_split=3)\n",
    "gdbtModel.fit(train_x_tmp, train_y)\n",
    "pResult = gdbtModel.predict(test_x_tmp)\n",
    "print(classification_report(test_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# joblib.dump(gdbtModel, '../production/model')\n",
    "# sum(pResult[-217:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(feature_names, gdbtModel.feature_importances_):\n",
    "    print (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'intense': train_x['cyclic_intense_downpeak'], 'label':train_y})\n",
    "dd1 = df1.groupby(['intense']).count().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd2 = df1.groupby(['intense']).sum().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd1['false_count'] = dd2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd1['ratio'] = dd1['false_count'] * 100.0 / dd1['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print len(feature_names)\n",
    "glue_df_x = features(glue_df, feature_names)\n",
    "glue_df_y = glue_df['expect_result']\n",
    "\n",
    "pResult = gdbtModel.predict(glue_df_x)\n",
    "print(classification_report(glue_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miss_classify_df = glue_df[pResult == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_features_x = features(miss_classify_df, feature_names)\n",
    "miss_features_x.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = miss_classify_df['case_path'][0]\n",
    "signals = pd.read_csv(path, skiprows=1, header=None)\n",
    "signals.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "normalized_signals = feas['normalized_signals'] \n",
    "medfiltered_signals = Filter.medfilter(normalized_signals, 9)\n",
    "peaks = normalized_signals - medfiltered_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(peaks).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Filter.nms(peaks, 9, True)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, len(peaks))[peaks < -1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas['down_peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "pd.Series(feas['normalized_signals']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "print (new_features['cyclic_downpeak_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_features['cyclic_intense_downpeak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(new_features['normalized_signals']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dtest = xgb.DMatrix(glue_df_x, label=glue_df_y)\n",
    "# predict = bst.predict(dtest)\n",
    "# # print(classification_report(test_y, pResult))\n",
    "# result = list()\n",
    "# for score in predict:\n",
    "#     if score >= 0.5:\n",
    "#         result.append(1)\n",
    "#     else:\n",
    "#         result.append(0)\n",
    "# print(classification_report(glue_df_y, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始拉须调研，判断更多的误差信息\n",
    "GLUE_ABNORMAL_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/DATA-胶水/'\n",
    "data_reader = DataReader()\n",
    "glue_df = data_reader.get_signal_list(GLUE_ABNORMAL_FPATH)\n",
    "glue_df['expect_result'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glue_df.head(100)['case_path']\n",
    "signals = pd.read_csv(path, skiprows=1)\n",
    "signals.plot()\n",
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "print (feas['cyclic_intense_downpeak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始胶水的波形调研，先看用现有的基线能得到多好的测试结果\n",
    "data_reader = DataReader()\n",
    "\n",
    "nohead_cyclic_intense = []\n",
    "downpeak_cyclic_intense = []\n",
    "for path in glue_df['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    nohead_cyclic_intense.append(feas['cyclic_intense_nopeak'])\n",
    "    downpeak_cyclic_intense.append(feas['cyclic_intense_downpeak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (np.mean(nohead_cyclic_intense), np.std(downpeak_cyclic_intense))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tpy2]",
   "language": "python",
   "name": "conda-env-tpy2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
